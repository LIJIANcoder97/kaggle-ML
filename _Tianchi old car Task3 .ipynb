{"cells":[{"metadata":{},"cell_type":"markdown","source":"### datawhale 2手车价格预测 特征工程"},{"metadata":{},"cell_type":"markdown","source":"目标\n- 处理异常值 \n- 处理缺失值\n- 归一化\n- 数据分桶\n- 构造新特征"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom operator import itemgetter\n\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"## 1) 载入训练集和测试集；\npath = '../input/usedcarpred/'\nTrain_data = pd.read_csv(path+'used_car_train_20200313.csv', sep=' ')\nTest_data = pd.read_csv(path+'used_car_testA_20200313.csv', sep=' ')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Train_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Train_data.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Test_data.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 异常值处理"},{"metadata":{"trusted":true},"cell_type":"code","source":"# 这里我包装了一个异常值处理的代码，可以随便调用。\ndef outliers_proc(data, col_name, scale=3):\n    \"\"\"\n    用于清洗异常值，默认用 box_plot（scale=3）进行清洗\n    :param data: 接收 pandas 数据格式\n    :param col_name: pandas 列名\n    :param scale: 尺度\n    :return:\n    \"\"\"\n\n    def box_plot_outliers(data_ser, box_scale):\n        \"\"\"\n        利用箱线图去除异常值\n        :param data_ser: 接收 pandas.Series 数据格式\n        :param box_scale: 箱线图尺度，\n        :return:\n        \"\"\"\n        iqr = box_scale * (data_ser.quantile(0.75) - data_ser.quantile(0.25))\n        val_low = data_ser.quantile(0.25) - iqr\n        val_up = data_ser.quantile(0.75) + iqr\n        rule_low = (data_ser < val_low)\n        rule_up = (data_ser > val_up)\n        return (rule_low, rule_up), (val_low, val_up)\n\n    data_n = data.copy()\n    data_series = data_n[col_name]\n    rule, value = box_plot_outliers(data_series, box_scale=scale)\n    index = np.arange(data_series.shape[0])[rule[0] | rule[1]]\n    print(\"Delete number is: {}\".format(len(index)))\n    data_n = data_n.drop(index)\n    data_n.reset_index(drop=True, inplace=True)\n    print(\"Now column number is: {}\".format(data_n.shape[0]))\n    index_low = np.arange(data_series.shape[0])[rule[0]]\n    outliers = data_series.iloc[index_low]\n    print(\"Description of data less than the lower bound is:\")\n    print(pd.Series(outliers).describe())\n    index_up = np.arange(data_series.shape[0])[rule[1]]\n    outliers = data_series.iloc[index_up]\n    print(\"Description of data larger than the upper bound is:\")\n    print(pd.Series(outliers).describe())\n    \n#     fig, ax = plt.subplots(1, 2, figsize=(10, 7))\n#     sns.boxplot(y=data[col_name], data=data, palette=\"Set1\", ax=ax[0])\n#     sns.boxplot(y=data_n[col_name], data=data_n, palette=\"Set1\", ax=ax[1])\n    return data_n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" - 对所有属性进行操作，删除异常值，\n - 异常场值是数字特征，所以先提取数字特征，\n - 因为Train_data有price，使用Test_data列明名"},{"metadata":{"trusted":true},"cell_type":"code","source":"numerical_features = Test_data.select_dtypes(exclude = [\"object\"]).columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"numerical_features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for name in numerical_features:\n    Train_data = outliers_proc(Train_data, name, scale=3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 构造新特征"},{"metadata":{"trusted":true},"cell_type":"code","source":"Train_data['train']=1\nTest_data['train']=0\ndata = pd.concat([Train_data, Test_data], ignore_index=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 使用时间：data['creatDate'] - data['regDate']，反应汽车使用时间，一般来说价格与使用时间成反比\n# 不过要注意，数据里有时间出错的格式，所以我们需要 errors='coerce'\ndata['used_time'] = (pd.to_datetime(data['creatDate'], format='%Y%m%d', errors='coerce') - \n                            pd.to_datetime(data['regDate'], format='%Y%m%d', errors='coerce')).dt.days","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 从邮编中提取城市信息，相当于加入了先验知识\ndata['city'] = data['regionCode'].apply(lambda x : str(x)[:-3])\ndata = data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Train_gb = Train_data.groupby(\"brand\")\nall_info = {}\nfor kind, kind_data in Train_gb:\n    info = {}\n    kind_data = kind_data[kind_data['price'] > 0]\n    info['brand_amount'] = len(kind_data)\n    info['brand_price_max'] = kind_data.price.max()\n    info['brand_price_median'] = kind_data.price.median()\n    info['brand_price_min'] = kind_data.price.min()\n    info['brand_price_sum'] = kind_data.price.sum()\n    info['brand_price_std'] = kind_data.price.std()\n    info['brand_price_average'] = round(kind_data.price.sum() / (len(kind_data) + 1), 2)\n    all_info[kind] = info\nbrand_fe = pd.DataFrame(all_info).T.reset_index().rename(columns={\"index\": \"brand\"})\ndata = data.merge(brand_fe, how='left', on='brand')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 看一下空数据，有 15k 个样本的时间是有问题的，我们可以选择删除，也可以选择放着。\n# 但是这里不建议删除，因为删除缺失数据占总样本量过大，7.5%\n# 我们可以先放着，因为如果我们 XGBoost 之类的决策树，其本身就能处理缺失值，所以可以不用管；\ndata['used_time'].isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = data.drop(['creatDate', 'regDate', 'regionCode'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 数据分桶"},{"metadata":{},"cell_type":"markdown","source":" 为什么要做数据分桶呢，原因有很多\n 1. 离散后稀疏向量内积乘法运算速度更快，计算结果也方便存储，容易扩展；\n 2. 离散后的特征对异常值更具鲁棒性，如 age>30 为 1 否则为 0，对于年龄为 200 的也不会对模型造成很大的干扰；\n 3. LR 属于广义线性模型，表达能力有限，经过离散化后，每个变量有单独的权重，这相当于引入了非线性，能够提升模型的表达能力，加大拟合；\n 4. 离散后特征可以进行特征交叉，提升表达能力，由 M+N 个变量编程 M*N 个变量，进一步引入非线形，提升了表达能力；\n 5. 特征离散后模型更稳定，如用户年龄区间，不会因为用户年龄长了一岁就变化\n\n还有很多原因，LightGBM 在改进 XGBoost 时就增加了数据分桶，增强了模型的泛化性"},{"metadata":{"trusted":true},"cell_type":"code","source":"bin = [i*10 for i in range(31)]\ndata['power_bin'] = pd.cut(data['power'], bin, labels=False)\ndata[['power_bin', 'power']].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(data.shape)\ndata.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 树模型数据导出\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.to_csv('data_for_tree.csv', index=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 线性回归模型数据"},{"metadata":{"trusted":true},"cell_type":"code","source":"# 之所以分开构造是因为，不同模型对数据集的要求不同\ndata['power'].plot.hist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Train_data['power'].plot.hist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Test_data['power'].plot.hist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 取 log，在做归一化\nfrom sklearn import preprocessing\nmin_max_scaler = preprocessing.MinMaxScaler()\ndata['power'] = np.log(data['power'] + 1) \ndata['power'] = ((data['power'] - np.min(data['power'])) / (np.max(data['power']) - np.min(data['power'])))\ndata['power'].plot.hist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# km 的比较正常，应该是已经做过分桶了\ndata['kilometer'].plot.hist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 所以可以直接做归一化\ndata['kilometer'] = ((data['kilometer'] - np.min(data['kilometer'])) / \n                        (np.max(data['kilometer']) - np.min(data['kilometer'])))\ndata['kilometer'].plot.hist()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"构造特征"},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef max_min(x):\n    return (x - np.min(x)) / (np.max(x) - np.min(x))\n\ndata['brand_amount'] = ((data['brand_amount'] - np.min(data['brand_amount'])) / \n                        (np.max(data['brand_amount']) - np.min(data['brand_amount'])))\ndata['brand_price_average'] = ((data['brand_price_average'] - np.min(data['brand_price_average'])) / \n                               (np.max(data['brand_price_average']) - np.min(data['brand_price_average'])))\ndata['brand_price_max'] = ((data['brand_price_max'] - np.min(data['brand_price_max'])) / \n                           (np.max(data['brand_price_max']) - np.min(data['brand_price_max'])))\ndata['brand_price_median'] = ((data['brand_price_median'] - np.min(data['brand_price_median'])) /\n                              (np.max(data['brand_price_median']) - np.min(data['brand_price_median'])))\ndata['brand_price_min'] = ((data['brand_price_min'] - np.min(data['brand_price_min'])) / \n                           (np.max(data['brand_price_min']) - np.min(data['brand_price_min'])))\ndata['brand_price_std'] = ((data['brand_price_std'] - np.min(data['brand_price_std'])) / \n                           (np.max(data['brand_price_std']) - np.min(data['brand_price_std'])))\ndata['brand_price_sum'] = ((data['brand_price_sum'] - np.min(data['brand_price_sum'])) / \n                           (np.max(data['brand_price_sum']) - np.min(data['brand_price_sum'])))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 对类别特征进行 OneEncoder\ndata = pd.get_dummies(data, columns=['model', 'brand', 'bodyType', 'fuelType',\n                                     'gearbox', 'notRepairedDamage', 'power_bin'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## LR数据导出"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.to_csv('data_for_lr.csv', index=0)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}